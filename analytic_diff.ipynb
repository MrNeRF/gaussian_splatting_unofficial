{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Function, gradcheck\n",
    "\n",
    "class CameraProject(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, y, z, fx, fy, cx, cy):\n",
    "        u = fx * x / z + cx\n",
    "        v = fy * y / z + cy\n",
    "        ctx.save_for_backward(x, y, z, fx, fy)\n",
    "        return u, v\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_u, grad_v):\n",
    "        x, y, z, fx, fy = ctx.saved_tensors\n",
    "        grad_x = grad_u * fx / z\n",
    "        grad_y = grad_v * fy / z\n",
    "        grad_z = -grad_u * fx * x / z**2 - grad_v * fy * y / z**2\n",
    "        return grad_x, grad_y, grad_z, None, None, None, None\n",
    "    \n",
    "x = torch.tensor(10.0, dtype=torch.float64, requires_grad=True)\n",
    "y = torch.tensor(-5.0, dtype=torch.float64, requires_grad=True)\n",
    "z = torch.tensor(10.0, dtype=torch.float64, requires_grad=True)\n",
    "fx = torch.tensor(1300.0, dtype=torch.float64, requires_grad=False)\n",
    "fy = torch.tensor(1200.0, dtype=torch.float64, requires_grad=False)\n",
    "cx = torch.tensor(320.0, dtype=torch.float64, requires_grad=False)\n",
    "cy = torch.tensor(240.0, dtype=torch.float64, requires_grad=False)\n",
    "\n",
    "# u, v = CameraProject.apply(x, y, z, fx, fy, cx, cy)\n",
    "\n",
    "test = gradcheck(CameraProject.apply, (x, y, z, fx, fy, cx, cy))\n",
    "print(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Function, gradcheck\n",
    "\n",
    "class MatrixMultiplication(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, A, B):\n",
    "        C = A @ B\n",
    "        ctx.save_for_backward(A, B)\n",
    "        return C\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_C):\n",
    "        A, B, = ctx.saved_tensors\n",
    "        grad_A = grad_C @ B.T\n",
    "        grad_B = A.T @ grad_C\n",
    "        return grad_A, grad_B\n",
    "\n",
    "    \n",
    "R = torch.rand(3, 3, dtype=torch.float64, requires_grad=True)\n",
    "S = torch.rand(3, 3, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "test = gradcheck(MatrixMultiplication.apply, (R, S))\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Function, gradcheck\n",
    "\n",
    "class RSSR(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, R, S):\n",
    "        RS = R @ S\n",
    "        RSSR = RS @ RS.T\n",
    "        ctx.save_for_backward(R, S)\n",
    "        return RSSR\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_RSSR):\n",
    "        R, S = ctx.saved_tensors\n",
    "        RS = R @ S\n",
    "        grad_RS = grad_RSSR @ RS\n",
    "        grad_SR = RS.T @ grad_RSSR\n",
    "\n",
    "        grad_R = grad_RS @ S.T\n",
    "        grad_S = R.T @ grad_RS\n",
    "\n",
    "        grad_R_t = S @ grad_SR\n",
    "        grad_S_t = grad_SR @ R\n",
    "\n",
    "\n",
    "        return grad_R + grad_R_t.T, grad_S + grad_S_t.T\n",
    "    \n",
    "\n",
    "R = torch.rand(3, 3, dtype=torch.float64, requires_grad=True)\n",
    "S = torch.rand(3, 3, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "test = gradcheck(RSSR.apply, (R, S))\n",
    "print(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Function, gradcheck\n",
    "\n",
    "class ComputeSigmaImage(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, sigma_world, W, J):\n",
    "        JW = J @ W\n",
    "        sigma_image = JW @ sigma_world @ JW.T\n",
    "        ctx.save_for_backward(sigma_world, W, J)\n",
    "        return sigma_image\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_sigma_image):\n",
    "        sigma_world, W, J = ctx.saved_tensors\n",
    "        JW = J @ W \n",
    "\n",
    "        # using First Quadratic Form 2.3.2 from: https://people.maths.ox.ac.uk/gilesm/files/NA-08-01.pdf\n",
    "        # for C = B_t @ A @ B \n",
    "        # grad_A = B @ grad_C @ B_t\n",
    "        # grad_B = A @ B @ grad_C_t + A_t @ B @ grad_C\n",
    "        \n",
    "        # applying to our variables\n",
    "        # sigma_image = JW @ sigma_world @ JW.T\n",
    "        # C = sigma_image\n",
    "        # A = sigma_world\n",
    "        # B = JW_t\n",
    "\n",
    "        grad_sigma_world = JW.T @ grad_sigma_image @ JW\n",
    "        grad_JW_t = sigma_world @ JW.T @ grad_sigma_image.T + sigma_world.T @ JW.T @ grad_sigma_image\n",
    "\n",
    "        # compute gradient of JW_t using multiplication rules in 2.2.2 \n",
    "        grad_W_t =  grad_JW_t @ J\n",
    "        grad_J_t = W @ grad_JW_t\n",
    "\n",
    "        grad_W = grad_W_t.T\n",
    "        grad_J = grad_J_t.T\n",
    "\n",
    "        return grad_sigma_world, grad_W, grad_J\n",
    "\n",
    "\n",
    "sigma_world = torch.rand(3, 3, dtype=torch.float64, requires_grad=True)\n",
    "W = torch.rand(3, 3, dtype=torch.float64, requires_grad=True)\n",
    "J = torch.rand(2, 3, dtype=torch.float64, requires_grad=True)\n",
    "test = gradcheck(ComputeSigmaImage.apply, (sigma_world, W, J))\n",
    "print(test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "from sympy import print_latex\n",
    "\n",
    "def quaternion_to_rotation_Symbolic(q):\n",
    "    # norm = sp.sqrt(q[0]**2 + x**2 + y**2 + z**2)\n",
    "    norm = 1\n",
    "    w = w / norm\n",
    "    x = x / norm\n",
    "    y = y / norm\n",
    "    z = z / norm\n",
    "    # Compute the rotation matrix\n",
    "    rotation_matrix = sp.Matrix([[1 - 2*y**2 - 2*z**2, 2*x*y - 2*w*z, 2*x*z + 2*w*y],\n",
    "                               [2*x*y + 2*w*z, 1 - 2*x**2 - 2*z**2, 2*y*z - 2*w*x],\n",
    "                               [2*x*z - 2*w*y, 2*y*z + 2*w*x, 1 - 2*x**2 - 2*y**2]])\n",
    "    \n",
    "    return rotation_matrix\n",
    "\n",
    "\n",
    "w, x, y, z = sp.Symbols('w x y z')\n",
    "q = [w, x, y, z]\n",
    "\n",
    "rotation_matrix = quaternion_to_rotation_Symbolic(q)\n",
    "rotation_derivative = sp.diff(rotation_matrix, z)\n",
    "print_latex(rotation_derivative)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Jacobian of Quaternion to Rotation Matrix without normalization\n",
    "$$  \\frac{\\partial q}{\\partial w} = \\left[\\begin{matrix}0 & - 2 z & 2 y\\\\2 z & 0 & - 2 x\\\\- 2 y & 2 x & 0\\end{matrix}\\right] $$\n",
    "\n",
    "$$  \\frac{\\partial q}{\\partial x} =  \\left[\\begin{matrix}0 & 2 y & 2 z\\\\2 y & - 4 x & - 2 w\\\\2 z & 2 w & - 4 x\\end{matrix}\\right] $$ \n",
    "\n",
    "$$  \\frac{\\partial q}{\\partial y} =  \\left[\\begin{matrix}- 4 y & 2 x & 2 w\\\\2 x & 0 & 2 z\\\\- 2 w & 2 z & - 4 y\\end{matrix}\\right] $$ \n",
    "\n",
    "$$  \\frac{\\partial q}{\\partial z} =  \\left[\\begin{matrix}- 4 z & - 2 w & 2 x\\\\2 w & - 4 z & 2 y\\\\2 x & 2 y & 0\\end{matrix}\\right] $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Function, gradcheck\n",
    "\n",
    "class QuaternionToRotation(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, q):\n",
    "        rot = [\n",
    "            1 - 2 * q[:, 2] ** 2 - 2 * q[:, 3] ** 2,\n",
    "            2 * q[:, 1] * q[:, 2] - 2 * q[:, 0] * q[:, 3],\n",
    "            2 * q[:, 3] * q[:, 1] + 2 * q[:, 0] * q[:, 2],\n",
    "            2 * q[:, 1] * q[:, 2] + 2 * q[:, 0] * q[:, 3],\n",
    "            1 - 2 * q[:, 1] ** 2 - 2 * q[:, 3] ** 2,\n",
    "            2 * q[:, 2] * q[:, 3] - 2 * q[:, 0] * q[:, 1],\n",
    "            2 * q[:, 3] * q[:, 1] - 2 * q[:, 0] * q[:, 2],\n",
    "            2 * q[:, 2] * q[:, 3] + 2 * q[:, 0] * q[:, 1],\n",
    "            1 - 2 * q[:, 1] ** 2 - 2 * q[:, 2] ** 2,\n",
    "        ]\n",
    "        rot = torch.stack(rot, dim=1).reshape(-1, 3, 3)\n",
    "        ctx.save_for_backward(q)\n",
    "        return rot\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_rot):\n",
    "        q = ctx.saved_tensors[0]\n",
    "\n",
    "        w = q[:, 0]\n",
    "        x = q[:, 1]\n",
    "        y = q[:, 2]\n",
    "        z = q[:, 3]\n",
    "\n",
    "        grad_qw = -2 * z *grad_rot[:,0 , 1] + 2 * y *grad_rot[:,0, 2] + 2 * z *grad_rot[:,1, 0] - 2 * x *grad_rot[:,1, 2] - 2 * y *grad_rot[:,2, 0] + 2 * x *grad_rot[:,2, 1]\n",
    "        grad_qx = 2 * y *grad_rot[:,0, 1] + 2 * z *grad_rot[:,0, 2] + 2 * y *grad_rot[:,1, 0] - 4 * x *grad_rot[:,1, 1] - 2 * w *grad_rot[:,1, 2] + 2 * z *grad_rot[:,2, 0] + 2 * w *grad_rot[:,2, 1] - 4 * x *grad_rot[:,2, 2]\n",
    "        grad_qy = -4 * y *grad_rot[:,0, 0] + 2 * x *grad_rot[:,0, 1] + 2 * w *grad_rot[:,0, 2] + 2 * x *grad_rot[:,1, 0] + 2 * z *grad_rot[:,1, 2] - 2 * w *grad_rot[:,2, 0] + 2 * z *grad_rot[:,2, 1] - 4 * y *grad_rot[:,2, 2]\n",
    "        grad_qz = -4 * z *grad_rot[:,0, 0] - 2 * w *grad_rot[:,0, 1] + 2 * x *grad_rot[:,0, 2] + 2 * w *grad_rot[:,1, 0] - 4 * z *grad_rot[:,1, 1] + 2 * y *grad_rot[:,1, 2] + 2 * x *grad_rot[:,2, 0] + 2 * y *grad_rot[:,2, 1]\n",
    "        grad_q = torch.stack([grad_qw, grad_qx, grad_qy, grad_qz], dim=1)\n",
    "\n",
    "        return grad_q\n",
    "        \n",
    "\n",
    "q = torch.rand(10, 4, dtype=torch.float64, requires_grad=True)\n",
    "norm_q = torch.norm(q, dim=1, keepdim=True)\n",
    "q = q / norm_q\n",
    "\n",
    "test = gradcheck(QuaternionToRotation.apply, (q))\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "from sympy import print_latex\n",
    "\n",
    "w, x, y, z = sp.symbols('w x y z')\n",
    "q = sp.Matrix([w, x, y, z])\n",
    "norm = sp.sqrt(w**2 + x**2 + y**2 + z**2)\n",
    "\n",
    "q_norm = q / norm\n",
    "\n",
    "dw = sp.diff(q_norm, w)\n",
    "dx = sp.diff(q_norm, x)\n",
    "dy = sp.diff(q_norm, y)\n",
    "dz = sp.diff(q_norm, z)\n",
    "\n",
    "print_latex(dw)\n",
    "print_latex(dx)\n",
    "print_latex(dy)\n",
    "print_latex(dz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partial Derivatives of Quaternion Normalization\n",
    "\n",
    "$$ \\frac{\\partial q}{\\partial w} = \\left[\\begin{matrix}- \\frac{w^{2}}{\\left(w^{2} + x^{2} + y^{2} + z^{2}\\right)^{\\frac{3}{2}}} + \\frac{1}{\\sqrt{w^{2} + x^{2} + y^{2} + z^{2}}}\\\\- \\frac{w x}{\\left(w^{2} + x^{2} + y^{2} + z^{2}\\right)^{\\frac{3}{2}}}\\\\- \\frac{w y}{\\left(w^{2} + x^{2} + y^{2} + z^{2}\\right)^{\\frac{3}{2}}}\\\\- \\frac{w z}{\\left(w^{2} + x^{2} + y^{2} + z^{2}\\right)^{\\frac{3}{2}}}\\end{matrix}\\right] $$\n",
    "\n",
    "$$ \\frac{\\partial q}{\\partial x} = \\left[\\begin{matrix}- \\frac{w x}{\\left(w^{2} + x^{2} + y^{2} + z^{2}\\right)^{\\frac{3}{2}}}\\\\- \\frac{x^{2}}{\\left(w^{2} + x^{2} + y^{2} + z^{2}\\right)^{\\frac{3}{2}}} + \\frac{1}{\\sqrt{w^{2} + x^{2} + y^{2} + z^{2}}}\\\\- \\frac{x y}{\\left(w^{2} + x^{2} + y^{2} + z^{2}\\right)^{\\frac{3}{2}}}\\\\- \\frac{x z}{\\left(w^{2} + x^{2} + y^{2} + z^{2}\\right)^{\\frac{3}{2}}}\\end{matrix}\\right] $$\n",
    "\n",
    "$$ \\frac{\\partial q}{\\partial y} =\\left[\\begin{matrix}- \\frac{w y}{\\left(w^{2} + x^{2} + y^{2} + z^{2}\\right)^{\\frac{3}{2}}}\\\\- \\frac{x y}{\\left(w^{2} + x^{2} + y^{2} + z^{2}\\right)^{\\frac{3}{2}}}\\\\- \\frac{y^{2}}{\\left(w^{2} + x^{2} + y^{2} + z^{2}\\right)^{\\frac{3}{2}}} + \\frac{1}{\\sqrt{w^{2} + x^{2} + y^{2} + z^{2}}}\\\\- \\frac{y z}{\\left(w^{2} + x^{2} + y^{2} + z^{2}\\right)^{\\frac{3}{2}}}\\end{matrix}\\right] $$ \n",
    "\n",
    "$$ \\frac{\\partial q}{\\partial z} = \\left[\\begin{matrix}- \\frac{w z}{\\left(w^{2} + x^{2} + y^{2} + z^{2}\\right)^{\\frac{3}{2}}}\\\\- \\frac{x z}{\\left(w^{2} + x^{2} + y^{2} + z^{2}\\right)^{\\frac{3}{2}}}\\\\- \\frac{y z}{\\left(w^{2} + x^{2} + y^{2} + z^{2}\\right)^{\\frac{3}{2}}}\\\\- \\frac{z^{2}}{\\left(w^{2} + x^{2} + y^{2} + z^{2}\\right)^{\\frac{3}{2}}} + \\frac{1}{\\sqrt{w^{2} + x^{2} + y^{2} + z^{2}}}\\end{matrix}\\right] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Function, gradcheck\n",
    "\n",
    "class QuaternionNormalization(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, q):\n",
    "        q_norm = q / torch.norm(q, dim=1, keepdim=True)\n",
    "        ctx.save_for_backward(q)\n",
    "        return q_norm\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_q_norm):\n",
    "        q = ctx.saved_tensors[0]\n",
    "        w = q[:, 0]\n",
    "        x = q[:, 1]\n",
    "        y = q[:, 2]\n",
    "        z = q[:, 3]\n",
    "        \n",
    "        norm_sq = w * w + x * x + y * y + z * z\n",
    "        grad_qw = (-1 * w * w / norm_sq**1.5 + 1/norm_sq**0.5) * grad_q_norm[:, 0] - w * x / norm_sq**1.5 * grad_q_norm[:, 1] - w * y / norm_sq**1.5 * grad_q_norm[:, 2] - w * z / norm_sq**1.5 * grad_q_norm[:, 3]\n",
    "        grad_qx = -w * x / norm_sq**1.5 * grad_q_norm[:, 0] + (-1 * x * x / norm_sq**1.5 + 1/norm_sq**0.5) * grad_q_norm[:, 1] - x * y / norm_sq**1.5 * grad_q_norm[:, 2] - x * z / norm_sq**1.5 * grad_q_norm[:, 3]\n",
    "        grad_qy = -w * y / norm_sq**1.5 * grad_q_norm[:, 0] - x * y / norm_sq**1.5 * grad_q_norm[:, 1] + (-1 * y * y / norm_sq**1.5 + 1/norm_sq**0.5) * grad_q_norm[:, 2] - y * z / norm_sq**1.5 * grad_q_norm[:, 3]\n",
    "        grad_qz = -w * z / norm_sq**1.5 * grad_q_norm[:, 0] - x * z / norm_sq**1.5 * grad_q_norm[:, 1] - y * z / norm_sq**1.5 * grad_q_norm[:, 2] + (-1 * z * z / norm_sq**1.5 + 1/norm_sq**0.5) * grad_q_norm[:, 3]\n",
    "        grad_q = torch.stack([grad_qw, grad_qx, grad_qy, grad_qz], dim=1)\n",
    "\n",
    "        return grad_q\n",
    "        \n",
    "\n",
    "q = torch.rand(2, 4, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "test = gradcheck(QuaternionNormalization.apply, (q))\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Function, gradcheck\n",
    "\n",
    "class ComputeSigmaWorld(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, q, scale):\n",
    "        S = torch.diag_embed(torch.exp(scale))\n",
    "        norm_q = torch.norm(q, dim=1, keepdim=True)\n",
    "        q_norm = q / norm_q\n",
    "        R = [\n",
    "            1 - 2 * q_norm[:, 2] ** 2 - 2 * q_norm[:, 3] ** 2,\n",
    "            2 * q_norm[:, 1] * q_norm[:, 2] - 2 * q_norm[:, 0] * q_norm[:, 3],\n",
    "            2 * q_norm[:, 3] * q_norm[:, 1] + 2 * q_norm[:, 0] * q_norm[:, 2],\n",
    "            2 * q_norm[:, 1] * q_norm[:, 2] + 2 * q_norm[:, 0] * q_norm[:, 3],\n",
    "            1 - 2 * q_norm[:, 1] ** 2 - 2 * q_norm[:, 3] ** 2,\n",
    "            2 * q_norm[:, 2] * q_norm[:, 3] - 2 * q_norm[:, 0] * q_norm[:, 1],\n",
    "            2 * q_norm[:, 3] * q_norm[:, 1] - 2 * q_norm[:, 0] * q_norm[:, 2],\n",
    "            2 * q_norm[:, 2] * q_norm[:, 3] + 2 * q_norm[:, 0] * q_norm[:, 1],\n",
    "            1 - 2 * q_norm[:, 1] ** 2 - 2 * q_norm[:, 2] ** 2,\n",
    "        ]\n",
    "        R = torch.stack(R, dim=1).reshape(-1, 3, 3)\n",
    "\n",
    "        RS = torch.bmm(R, S)\n",
    "        RS_t = RS.permute(0, 2, 1)\n",
    "\n",
    "        RSSR = torch.bmm(RS, RS_t)\n",
    "        ctx.save_for_backward(RS, R, S, scale, q, q_norm)\n",
    "        return RSSR\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_RSSR):\n",
    "        # compute double matmul gradient        \n",
    "        RS, R, S, scale, q, q_norm = ctx.saved_tensors\n",
    "        grad_RS = torch.bmm(grad_RSSR, RS)\n",
    "        \n",
    "        RS_t = RS.permute(0, 2, 1)\n",
    "        grad_SR = RS_t @ grad_RSSR\n",
    "\n",
    "        grad_R = grad_RS @ S.permute(0, 2, 1) + (S @ grad_SR).permute(0, 2, 1)\n",
    "        grad_S = R.permute(0, 2, 1) @ grad_RS + (grad_SR @ R).permute(0, 2, 1)\n",
    "\n",
    "        # compute quaternion gradient\n",
    "        w = q_norm[:, 0]\n",
    "        x = q_norm[:, 1]\n",
    "        y = q_norm[:, 2]\n",
    "        z = q_norm[:, 3]\n",
    "        grad_qw_norm = -2 * z *grad_R[:,0 , 1] + 2 * y *grad_R[:,0, 2] + 2 * z *grad_R[:,1, 0] - \\\n",
    "            2 * x *grad_R[:,1, 2] - 2 * y *grad_R[:,2, 0] + 2 * x *grad_R[:,2, 1]\n",
    "        grad_qx_norm = 2 * y *grad_R[:,0, 1] + 2 * z *grad_R[:,0, 2] + 2 * y *grad_R[:,1, 0] - \\\n",
    "            4 * x *grad_R[:,1, 1] - 2 * w *grad_R[:,1, 2] + 2 * z *grad_R[:,2, 0] + 2 * w *grad_R[:,2, 1] - 4 * x *grad_R[:,2, 2]\n",
    "        grad_qy_norm = -4 * y *grad_R[:,0, 0] + 2 * x *grad_R[:,0, 1] + 2 * w *grad_R[:,0, 2] + \\\n",
    "            2 * x *grad_R[:,1, 0] + 2 * z *grad_R[:,1, 2] - 2 * w *grad_R[:,2, 0] + 2 * z *grad_R[:,2, 1] - 4 * y *grad_R[:,2, 2]\n",
    "        grad_qz_norm = -4 * z *grad_R[:,0, 0] - 2 * w *grad_R[:,0, 1] + 2 * x *grad_R[:,0, 2] + \\\n",
    "            2 * w *grad_R[:,1, 0] - 4 * z *grad_R[:,1, 1] + 2 * y *grad_R[:,1, 2] + 2 * x *grad_R[:,2, 0] + 2 * y *grad_R[:,2, 1]\n",
    "        grad_q_norm = torch.stack([grad_qw_norm, grad_qx_norm, grad_qy_norm, grad_qz_norm], dim=1)\n",
    "\n",
    "        # compute gradient for unnormalized quaternion\n",
    "        w = q[:, 0]\n",
    "        x = q[:, 1]\n",
    "        y = q[:, 2]\n",
    "        z = q[:, 3]\n",
    "        norm_sq = w * w + x * x + y * y + z * z\n",
    "        grad_qw = (-1 * w * w / norm_sq**1.5 + 1/norm_sq**0.5) * grad_q_norm[:, 0] - w * x / norm_sq**1.5 * grad_q_norm[:, 1] - \\\n",
    "            w * y / norm_sq**1.5 * grad_q_norm[:, 2] - w * z / norm_sq**1.5 * grad_q_norm[:, 3]\n",
    "        grad_qx = -w * x / norm_sq**1.5 * grad_q_norm[:, 0] + (-1 * x * x / norm_sq**1.5 + 1/norm_sq**0.5) * grad_q_norm[:, 1] - \\\n",
    "            x * y / norm_sq**1.5 * grad_q_norm[:, 2] - x * z / norm_sq**1.5 * grad_q_norm[:, 3]\n",
    "        grad_qy = -w * y / norm_sq**1.5 * grad_q_norm[:, 0] - x * y / norm_sq**1.5 * grad_q_norm[:, 1] + (-1 * y * y / norm_sq**1.5 + \\\n",
    "            1/norm_sq**0.5) * grad_q_norm[:, 2] - y * z / norm_sq**1.5 * grad_q_norm[:, 3]\n",
    "        grad_qz = -w * z / norm_sq**1.5 * grad_q_norm[:, 0] - x * z / norm_sq**1.5 * grad_q_norm[:, 1] - y * z / norm_sq**1.5 * grad_q_norm[:, 2] + \\\n",
    "            (-1 * z * z / norm_sq**1.5 + 1/norm_sq**0.5) * grad_q_norm[:, 3]\n",
    "        grad_q = torch.stack([grad_qw, grad_qx, grad_qy, grad_qz], dim=1)\n",
    "\n",
    "        grad_scale_no_activation = grad_S.diagonal(dim1=1, dim2=2)\n",
    "        grad_scale = grad_scale_no_activation * torch.exp(scale)\n",
    "\n",
    "        return grad_q, grad_scale\n",
    "    \n",
    "N = 2\n",
    "q = torch.rand(N, 4, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "s = torch.rand(N, 3, dtype=torch.float64, requires_grad=True)\n",
    "test = gradcheck(ComputeSigmaWorld.apply, (q, s))\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\frac{2 a v - b u - c u}{a d - b c}\n"
     ]
    }
   ],
   "source": [
    "import sympy as sp\n",
    "from sympy import print_latex, exp\n",
    "\n",
    "\n",
    "a, b, c, d = sp.symbols('a b c d')\n",
    "sigma_image = sp.Matrix([[a, b], [c, d]])\n",
    "\n",
    "u, v = sp.symbols('u v')\n",
    "\n",
    "mh_dist_sq = (d * u ** 2 - b * u * v - c * u * v + a * v ** 2) / (a * d - b * c)\n",
    "\n",
    "print_latex(sp.diff(mh_dist_sq, v))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ d_m^2 = \\frac{a v^{2} - b u v - c u v + d u^{2}}{a d - b c} $$ \n",
    "\n",
    "$$ \\frac{\\partial d_m^2}{\\partial a} = - \\frac{d \\left(a v^{2} - b u v - c u v + d u^{2}\\right)}{\\left(a d - b c\\right)^{2}} + \\frac{v^{2}}{a d - b c} $$ \n",
    "\n",
    "$$ \\frac{\\partial d_m^2}{\\partial b} = \\frac{c \\left(a v^{2} - b u v - c u v + d u^{2}\\right)}{\\left(a d - b c\\right)^{2}} - \\frac{u v}{a d - b c} $$ \n",
    "\n",
    "$$ \\frac{\\partial d_m^2}{\\partial c} = \\frac{b \\left(a v^{2} - b u v - c u v + d u^{2}\\right)}{\\left(a d - b c\\right)^{2}} - \\frac{u v}{a d - b c} $$ \n",
    "\n",
    "$$ \\frac{\\partial d_m^2}{\\partial d} = - \\frac{a \\left(a v^{2} - b u v - c u v + d u^{2}\\right)}{\\left(a d - b c\\right)^{2}} + \\frac{u^{2}}{a d - b c} $$\n",
    "\n",
    "\n",
    "$$ \\frac{\\partial d_m^2}{\\partial u} = \\frac{- b v - c v + 2 d u}{a d - b c} $$ \n",
    "\n",
    "$$ \\frac{\\partial d_m^2}{\\partial v} = \\frac{2 a v - b u - c u}{a d - b c} $$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Function, gradcheck\n",
    "\n",
    "class ComputeAlpha(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, sigma_image, opa, uv_splat, uv_pixel):\n",
    "        uv_diff = uv_pixel - uv_splat \n",
    "        a = sigma_image[0, 0]\n",
    "        b = sigma_image[0, 1]\n",
    "        c = sigma_image[1, 0]\n",
    "        d = sigma_image[1, 1]\n",
    "        mh_dist = (d * uv_diff[0] ** 2 - b * uv_diff[0] * uv_diff[1] - c * uv_diff[0] * uv_diff[1] + a * uv_diff[1] ** 2) / (a * d - b * c)\n",
    "\n",
    "        prob = torch.exp(-0.5 * mh_dist)\n",
    "        alpha = prob * opa\n",
    "        ctx.save_for_backward(prob, sigma_image, uv_diff, opa)\n",
    "        return alpha\n",
    "        \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_alpha):\n",
    "        prob, sigma_image, uv_diff, opa = ctx.saved_tensors\n",
    "        grad_opa = prob * grad_alpha\n",
    "\n",
    "        ## compute sigma world and uv_diff gradients        \n",
    "        grad_prob = opa * grad_alpha\n",
    "        grad_mh = -0.5 * prob * grad_prob\n",
    "\n",
    "        a = sigma_image[0, 0]\n",
    "        b = sigma_image[0, 1]\n",
    "        c = sigma_image[1, 0]\n",
    "        d = sigma_image[1, 1]\n",
    "\n",
    "        u_diff = uv_diff[0]\n",
    "        v_diff = uv_diff[1]\n",
    "\n",
    "        grad_u = -(-b * v_diff - c * v_diff + 2 * d * u_diff) / (a * d - b * c) * grad_mh\n",
    "        grad_v = -(2 * a * v_diff - b * u_diff - c * u_diff) / (a * d - b * c) * grad_mh\n",
    "\n",
    "        grad_a = (-d * (a * v_diff ** 2 - b * u_diff * v_diff - c * u_diff * v_diff + d * u_diff ** 2) / (a * d - b * c)**2  + (v_diff ** 2) / (a * d - b * c)) * grad_mh\n",
    "        grad_b = (c * (a * v_diff ** 2 - b * u_diff * v_diff - c * u_diff * v_diff + d * u_diff ** 2) / (a * d - b * c)**2  - (u_diff * v_diff) / (a * d - b * c)) * grad_mh\n",
    "        grad_c = (b * (a * v_diff ** 2 - b * u_diff * v_diff - c * u_diff * v_diff + d * u_diff ** 2) / (a * d - b * c)**2  - (u_diff * v_diff) / (a * d - b * c)) * grad_mh\n",
    "        grad_d = (-a * (a * v_diff ** 2 - b * u_diff * v_diff - c * u_diff * v_diff + d * u_diff ** 2) / (a * d - b * c)**2  + (u_diff ** 2) / (a * d - b * c)) * grad_mh\n",
    "\n",
    "        grad_sigma_image = torch.Tensor([[grad_a, grad_b], [grad_c, grad_d]])\n",
    "        grad_uv_splat = torch.Tensor([grad_u, grad_v])\n",
    "\n",
    "        return grad_sigma_image, grad_opa, grad_uv_splat, None\n",
    "\n",
    "\n",
    "uv_splat = torch.rand(2, dtype=torch.float64, requires_grad=True)\n",
    "uv_pixel = torch.rand(2, dtype=torch.float64, requires_grad=False)\n",
    "\n",
    "sigma_image = torch.rand(2, 2, dtype=torch.float64, requires_grad=True)\n",
    "opa = torch.rand(1, dtype=torch.float64, requires_grad=True)\n",
    "test = gradcheck(ComputeAlpha.apply, (sigma_image, opa, uv_splat, uv_pixel))\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alpha Compositing\n",
    "\n",
    "First (front-to-back) Gaussian Splatted to Pixel: \n",
    "$$ \\alpha_a = 0.0 $$ \n",
    "$$ \\alpha_c = \\alpha_0(1.0 - \\alpha_a) = \\alpha_0 $$ \n",
    "\n",
    "Second splat:\n",
    "$$ \\alpha_a = \\alpha_0 $$\n",
    "$$ \\alpha_c = \\alpha_1(1.0 - \\alpha_a) = \\alpha_1(1.0 - \\alpha_0) $$ \n",
    "\n",
    "Third splat:\n",
    "$$ \\alpha_a = \\alpha_0 + \\alpha_1(1.0 - \\alpha_0) $$\n",
    "$$ \\alpha_c = \\alpha_2(1.0 - \\alpha_a) = \\alpha_2(1.0 - (\\alpha_0 + \\alpha_1(1.0 - \\alpha_0))) $$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Function, gradcheck\n",
    "\n",
    "class AlphaComposite(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, colors, alphas):\n",
    "        alpha_accum = 0.0\n",
    "        color_accum = torch.zeros_like(colors[0])\n",
    "        for i in range(alphas.shape[0]):\n",
    "            alpha_weight = (1 - alpha_accum)\n",
    "            alpha_current = alphas[i] * (1 - alpha_accum)\n",
    "            color_accum += alpha_current * colors[i, :]\n",
    "            alpha_accum += alpha_current\n",
    "\n",
    "        ctx.save_for_backward(alpha_weight, alphas, colors)\n",
    "        return color_accum\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_color_accum):\n",
    "        weight_final, alphas, colors = ctx.saved_tensors\n",
    "        grad_alphas = torch.zeros_like(alphas)\n",
    "        grad_colors = torch.zeros_like(colors)\n",
    "\n",
    "        colors_accum = torch.zeros_like(colors[0])\n",
    "        weight = weight_final\n",
    "        for i in reversed(range(alphas.shape[0])):\n",
    "            grad_colors[i] = alphas[i] * weight * grad_color_accum\n",
    "            grad_alphas[i] = torch.dot((colors[i, :] * weight - colors_accum/(1.0 - alphas[i])), grad_color_accum)\n",
    "\n",
    "            colors_accum += alphas[i] * colors[i, :] * weight\n",
    "            weight /= (1 - alphas[i - 1])\n",
    "            \n",
    "        return grad_colors, grad_alphas\n",
    "\n",
    "alphas = torch.rand(10, dtype=torch.float64, requires_grad=True) / 10.0\n",
    "colors = torch.rand(10, 3, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "test = gradcheck(AlphaComposite.apply, (colors, alphas))\n",
    "print(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
